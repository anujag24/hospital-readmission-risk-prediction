{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all cause 30-day hospital readmission risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand the relationship between different tables and the data in those tables. This is important to identify the information which is relevant to the prediction. The tool that you used to generate the data created different csv files which you will upload to S3 bucket. Based on the generated data, you can see the below relationship between different tables within your data set. If you are using your own data for this notebook, it will help to create some visualization of the data to better understand the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Visualizing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EHR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in this machine learning project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding of your data \n",
    "2. Storing and converting your data into parquet for optimized performance and storage\n",
    "3. Feature selection and feature engineering using Spark\n",
    "4. Data pre-processing - StringIndexer and OneHotEncoding to convert categorical variables into required training data\n",
    "5. Train Spark ML model for data pre-processing and serialize using MLeap library to be used during inference pipeline\n",
    "6. Convert the data set into XGBoost supported format i.e. CSV from Spark Data Frame\n",
    "7. Split the data set into training and validation for model training and validation\n",
    "8. Train XGBoost Model using SageMaker XGBoost algorithm and validate model prediction using validation data set\n",
    "9. Tune the trained model using Hyperparameter tuning jobs for required HPO parameters\n",
    "10. Get the best tuned model and create inference pipeline which includes Spark ML model and XGBoost Model\n",
    "11. Create the end point configuration to deploy the inference pipeline\n",
    "12. Deploy the inference pipeline for real time prediction\n",
    "13. Invoke real time prediction API for a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to update the S3 Bucket and KMS Key Id with the values for your environment. This notebook requires certain resources to be created. Cloud Formation template has been provided to create the required resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore \n",
    "import time\n",
    "\n",
    "bucket = '' # Update this to the bucket that was created in your lab account as part of this enviroment.\n",
    "sse_kms_id = '' ## Update this value from Cloud Formation template\n",
    "glue_crawler_db = '' ## Update this value from Cloud Formation template\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing using Apache Spark in AWS Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Glue Scripts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download Dependencies\n",
    "wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Uploading Glue scripts and dependencies to S3\n",
    "from sagemaker import Session as Sess\n",
    "\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "result = sess.upload_data(path='glue_scripts/convert_to_parquet', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='glue_scripts/produce_training_data', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='python.zip', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='mleap_spark_assembly.jar', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your S3 Bucket is now ready with raw data and required scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run AWS Glue Preprocessing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll be creating Glue client via Boto3 so that we can invoke the start_job_run API of Glue. This API creates an immutable run/execution corresponding to the job definition created above. We will require the job_run_id for the particular job execution to check for status. We'll pass the data and model locations as part of the job execution parameters.\n",
    "\n",
    "Finally, we will check for the job status to see if it has succeeded, failed or stopped. Once the job is succeeded, we have the transformed data into S3 in required format. If the job fails, you can go to AWS Glue console, click on Jobs tab on the left, and from the page, click on this particular job and you will be able to find the CloudWatch logs (the link under Logs) link for these jobs which can help you to see what exactly went wrong in the job execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start CSV to Parquet conversion Glue Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Glue Job is setup to use Spark 2.4 and Python 3.0. The job requires a python script which is uploaded to the S3 bucket and provided to the job while creating this job using Cloud Formation template. You can generate these scripts in Glue using the console so that you don't have to write the script from scratch and can make modifications to the generated script as per your use case. In this case, we generated the script to read the data from Glue crawler database and then selecting only the required columns based on domain knowledge for pre-processing and model training. We will drop the null values and update the data types to be supported by our machine learning algorithm. Finally, the data set is saved to S3 bucket in parquet with partition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and run AWS Glue Preprocessing Job\n",
    "\n",
    "# Define the Job in AWS Glue\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "try:\n",
    "    glue.get_job(JobName='glue-etl-convert-to-parquet')\n",
    "    print(\"Job already exists, continuing...\")\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print('{}\\n'.format(\"Job Not Found, Check the output of Cloud Formation template\"))\n",
    "\n",
    "# Run the job in AWS Glue\n",
    "try:\n",
    "    job_name='glue-etl-convert-to-parquet'\n",
    "    response = glue.start_job_run(JobName=job_name,\n",
    "                                  Arguments={\n",
    "                                            '--s3_bucket' : bucket,\n",
    "                                            '--glue_crawler_db' : glue_crawler_db ##This value is from cloud formation template\n",
    "                                    })\n",
    "    job_run_id = response['JobRunId']\n",
    "    print('{}\\n'.format(response))\n",
    "except glue.exceptions.ConcurrentRunsExceededException:\n",
    "    print(\"Job run already in progress, continuing...\")\n",
    "\n",
    "    \n",
    "# Check on the job status\n",
    "import time\n",
    "\n",
    "job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Glue Job to Produce Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Glue Job is setup uses Spark 2.2 and Python 2.0. The job requires a python script which is uploaded to the S3 bucket and provided to the job while creating this job using Cloud Formation template. You can generate these scripts in Glue using the console so that you don't have to write the script from scratch and can make modifications to the generated script as per your use case. In this case, we are using Spark 2.2 instead of latest supported Spark version i.e. Spark 2.4 since MLeap serialization libraries provided for serializing Spark ML model for data pre-processing does not support Spark 2.4. You can check more details about this on https://github.com/aws/sagemaker-sparkml-serving-container. In the script, we are reading directly from S3 bucket all the partitions but you can filter partitions to read specific partitions. Here the partitions are based on the date but you can define your own partition strategy. As per the understanding of the data, you will join multiple tables to produce the training data for your machine learning model. After joining the tables, we will drop the columns which are not required. Since you are using Supervised learning model i.e. XGBoost we need to provide the label data. We will calculate label data i.e. 30-day readmission by sorting all the encounters by timestamp for a specific patient id and then taking a difference of encounter start from previous encounter stop. This will provide the number of days from last encounter and can be used to identify if the encounter was within last 30 days or not. After which we are using Imputation technique to fill some of the missing values in the data. We are also doing feature engineering to convert birth date into age to better suit our machine learning model. Once all this is done, you will generate features vector by leveraging Spark ML OneHotEncoding and then serialize the model using MLeap serialization library. Since XGBoost algorithm supports CSV format for training data, you need to convert Spark Data Frame into CSV files and save to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and run AWS Glue Preprocessing Job\n",
    "\n",
    "# Define the Job in AWS Glue\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "try:\n",
    "    glue.get_job(JobName='glue-etl-produce-traing-data')\n",
    "    print(\"Job already exists, continuing...\")\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print('{}\\n'.format(\"Job Not Found, Check the output of Cloud Formation template\"))\n",
    "\n",
    "# Run the job in AWS Glue\n",
    "try:\n",
    "    job_name='glue-etl-produce-traing-data'\n",
    "    response = glue.start_job_run(JobName=job_name,\n",
    "                                  Arguments={\n",
    "                                            '--sse_kms_id': sse_kms_id,\n",
    "                                            '--s3_bucket' : bucket\n",
    "                                    })\n",
    "    job_run_id = response['JobRunId']\n",
    "    print('{}\\n'.format(response))\n",
    "except glue.exceptions.ConcurrentRunsExceededException:\n",
    "    print(\"Job run already in progress, continuing...\")\n",
    "\n",
    "    \n",
    "# Check on the job status\n",
    "import time\n",
    "\n",
    "job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### [OPTIONAL] Validate Spark ML prediction\n",
    "You can use  another Jupyter notebook i.e. **Sparkml-model-test.ipynb** provided in this Github repo to understand and validate the prediction generated by Spark ML model for data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Amazon SageMaker XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data preprocessed in a format that XGBoost recognizes, we can run a simple training job to train a binary classifier model on our data. We can run this entire process in our Jupyter notebook. Run the following cell, labeled Run Amazon SageMaker XGBoost Training Job. This will run our XGBoost training job in Amazon SageMaker, and monitor the progress of the job. Once the job is ‘Completed’, you can move on to the next cell.\n",
    "\n",
    "This will train the model on the preprocessed data we created earlier. After a few minutes, usually less than 5, the job should complete successfully, and output our model artifacts to the S3 location we specified. Once this is done, we can deploy an inference pipeline that consists of pre-processing, inference and post-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use SageMaker XGBoost algorithm to train on this dataset. We already know the S3 location\n",
    "where the preprocessed training data was uploaded as part of the Glue job. You need to update train_prefix and validation_prefix with S3 prefix location of training and validation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to retrieve the XGBoost algorithm image\n",
    "We will retrieve the XGBoost built-in algorithm image so that it can leveraged for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker import Session as Sess\n",
    "\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "# Boto3 session\n",
    "session = boto3.session.Session()\n",
    "role = get_execution_role()\n",
    "region = session.region_name\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'xgboost', repo_version=\"latest\")\n",
    "print (training_image)\n",
    "\n",
    "##Get training and validation data set location on S3\n",
    "train_prefix = \"train-data/2020/3/13\" ## Update S3 Prefix\n",
    "validation_prefix = \"validation-data/2020/3/13\" ## Update S3 Prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next XGBoost model parameters and dataset details will be set properly\n",
    "We have parameterized this Notebook so that the same data location which was used in the PySpark script can now be passed to XGBoost Estimator as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Get XGBoost container image for current region\n",
    "training_image = get_image_uri(region, 'xgboost', repo_version=\"latest\")\n",
    "\n",
    "# Create a unique training job name\n",
    "training_job_name = 'xgboost-readmission-'+''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))\n",
    "\n",
    "# Create the training job in Amazon SageMaker\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "response = sagemaker.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    HyperParameters={\n",
    "        'early_stopping_rounds ': '5',\n",
    "        'num_round': '10',\n",
    "        'objective': 'binary:logistic', ## Binary classification since readmission will be Yes or NO. Get probability of binary classification\n",
    "        'eval_metric': 'auc'\n",
    "\n",
    "    },\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingImage': training_image,\n",
    "        'TrainingInputMode': 'File',\n",
    "    },\n",
    "    RoleArn=role,\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}'.format(bucket+'/'+train_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None',\n",
    "            'InputMode': 'File'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'validation',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}'.format(bucket+'/'+validation_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None',\n",
    "            'InputMode': 'File'\n",
    "        },\n",
    "    ],\n",
    "    OutputDataConfig={\n",
    "        'S3OutputPath': 's3://{}/xgb'.format(bucket),\n",
    "        'KmsKeyId' : sse_kms_id\n",
    "    },\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.m5.4xlarge', ## For XGBoost use memory optimized instances since all the data is loaded into memory so we need memory intensive Ec2 instance\n",
    "        'InstanceCount': 2, ## Distributed training\n",
    "        'VolumeSizeInGB': 10\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    },)\n",
    "\n",
    "print('{}\\n'.format(response))\n",
    "\n",
    "# Monitor the status until completed\n",
    "job_run_status = sagemaker.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    job_run_status = sagemaker.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning to find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Get XGBoost container image for current region\n",
    "training_image = get_image_uri(region, 'xgboost', repo_version=\"latest\")\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}'.format(bucket+'/'+train_prefix)\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}'.format(bucket+'/'+validation_prefix)\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/xgb\".format(bucket),\n",
    "      \"KmsKeyId\" : sse_kms_id\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 2, ## Distributed training\n",
    "      \"InstanceType\": \"ml.m5.4xlarge\",\n",
    "      \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"auc\",\n",
    "      \"num_round\": \"100\",\n",
    "      \"objective\": \"binary:logistic\",\n",
    "      \"rate_drop\": \"0.3\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 3600\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\",            \n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_depth\",\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 5,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:auc\",\n",
    "      \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique training job name\n",
    "import random\n",
    "import string\n",
    "\n",
    "tuning_job_name = 'xgboost-readmission-'+''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))\n",
    "\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the status until completed\n",
    "job_run_status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    job_run_status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying an Amazon SageMaker Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a set of model artifacts, we can set up an inference pipeline that executes sequentially in Amazon SageMaker. We start by setting up a Model, which will point to all of our model artifacts, then we setup an Endpoint configuration to specify our hardware, and finally we can stand up an Endpoint. With this endpoint, we will pass the raw data and no longer need to write pre-processing logic in our application code. The same pre-processing steps that ran for training can be applied to inference input data for better consistency and ease of management.\n",
    "\n",
    "Deploying a model in SageMaker requires two components:\n",
    "\n",
    "Docker image residing in ECR.\n",
    "Model artifacts residing in S3.\n",
    "SparkML\n",
    "\n",
    "For SparkML, Docker image for MLeap based SparkML serving is provided by SageMaker team. For more information on this, please see SageMaker SparkML Serving. MLeap serialized SparkML model was uploaded to S3 as part of the SparkML job we executed in AWS Glue.\n",
    "\n",
    "XGBoost\n",
    "\n",
    "For XGBoost, we will use the same Docker image we used for training. The model artifacts for XGBoost was uploaded as part of the training job we just ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Inference Pipeline consisting of SparkML & XGBoost models for a realtime inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the best training job name \n",
    "best_training_job = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['BestTrainingJob']['TrainingJobName']\n",
    "print ('Best training job : ' + best_training_job)\n",
    "\n",
    "info = smclient.describe_training_job(TrainingJobName=best_training_job)\n",
    "best_model_data_loc = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model Artifact Location : ' + best_model_data_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the schema of the payload via environment variable\n",
    "SparkML serving container needs to know the schema of the request that'll be passed to it while calling the `predict` method. In order to alleviate the pain of not having to pass the schema with every request, `sagemaker-sparkml-serving` allows you to pass it via an environment variable while creating the model definitions. This schema definition will be required in our next step for creating a model.\n",
    "\n",
    "You can overwrite this schema on a per request basis by passing it as part of the individual request payload as well. If you would like to explore on how to specify schema for each request, you can visit - https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone/sparkml_serving_emr_mleap_abalone.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema = {\"input\":[{\"type\":\"string\",\"name\":\"encounters_encounterclass\"},{\"type\":\"string\",\"name\":\"patient_gender\"},{\"type\":\"string\",\"name\":\"patient_marital\"},{\"type\":\"string\",\"name\":\"patient_ethnicity\"},{\"type\":\"string\",\"name\":\"patient_race\"},{\"type\":\"string\",\"name\":\"providers_speciality\"},{\"type\":\"string\",\"name\":\"encounters_reasoncode\"},{\"type\":\"string\",\"name\":\"encounters_code\"},{\"type\":\"string\",\"name\":\"procedures_code\"},{\"type\":\"double\",\"name\":\"patient_healthcare_expenses\"},{\"type\":\"double\",\"name\":\"patient_healthcare_coverage\"},{\"type\":\"double\",\"name\":\"encounters_total_claim_cost\"},{\"type\":\"double\",\"name\":\"encounters_payer_coverage\"},{\"type\":\"double\",\"name\":\"encounters_base_encounter_cost\"},{\"type\":\"double\",\"name\":\"procedures_base_cost\"},{\"type\":\"long\",\"name\":\"providers_utilization\"},{\"type\":\"double\",\"name\":\"age\"}],\"output\":{\"type\":\"double\",\"name\":\"features\",\"struct\":\"vector\"}}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `PipelineModel` which comprises of the SparkML and XGBoost model in the right order\n",
    "\n",
    "Next we'll create a SageMaker `PipelineModel` with SparkML and XGBoost.The `PipelineModel` will ensure that both the containers get deployed behind a single API endpoint in the correct order. The same model would later be used for Batch Transform as well to ensure that a single job is sufficient to do prediction against the Pipeline. \n",
    "\n",
    "Here, during the `Model` creation for SparkML, we will pass the schema definition that we built in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "sparkml_model_prefix = 'spark-ml-model/2020/3/13'\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(bucket,sparkml_model_prefix,'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model_data = '{}'.format(best_model_data_loc)\n",
    "xgb_model = Model(model_data=xgb_model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-readmission-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the `PipelineModel` to an endpoint for realtime inference\n",
    "Next we will deploy the model we just created with the `deploy()` method to start an inference endpoint and we will send some requests to the endpoint to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'inference-pipeline-readmission-ep-' + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.m5.4xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the status until completed\n",
    "endpoint_status = sagemaker.describe_endpoint(EndpointName='inference-pipeline-readmission-ep-' + timestamp_prefix)['EndpointStatus']\n",
    "while endpoint_status not in ('OutOfService','InService','Failed'):\n",
    "    endpoint_status = sagemaker.describe_endpoint(EndpointName='pipeline-xgboost-readmission')['EndpointStatus']\n",
    "    print(endpoint_status)\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the newly created inference endpoint with a payload to transform the data\n",
    "Now we will invoke the endpoint with a valid payload that SageMaker SparkML Serving can recognize. There are three ways in which input payload can be passed to the request:\n",
    "\n",
    "* Pass it as a valid CSV string. In this case, the schema passed via the environment variable will be used to determine the schema. For CSV format, every column in the input has to be a basic datatype (e.g. int, double, string) and it can not be a Spark `Array` or `Vector`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in CSV format\n",
    "You will first see get the test data to get the prediction validated. Get the test data values for readmission yes and no. Use these values in the request to get the prediction. You will see how the payload can be passed to the endpoint in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Test CSV into pandas\n",
    "import pandas as pd \n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "test_data_prefix = 'test-data/2020/3/13'\n",
    "\n",
    "# Python 3.6 or later\n",
    "p_dataset = pq.ParquetDataset(\n",
    "    f\"s3://{bucket}/{test_data_prefix}\",\n",
    "    filesystem=fs\n",
    ")\n",
    "\n",
    "test_data = p_dataset.read().to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_0 = test_data[(test_data['readmission'] == 0) & (test_data['encounters_reasoncode'] != 0) & (test_data['procedures_code'] != 0)]\n",
    "test_data_0.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = test_data[(test_data['readmission'] == 1) & (test_data['encounters_reasoncode'] != 0) & (test_data['procedures_code'] != 0)]\n",
    "test_data_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "## Payload schema = encounters_encounterclass,patient_gender,patient_marital,patient_ethnicity,patient_race,\n",
    "## providers_speciality,encounters_reasoncode,encounters_code,procedures_code,patient_healthcare_expenses,\n",
    "## patient_healthcare_coverage,encounters_total_claim_cost,encounters_payer_coverage,encounters_base_encounter_cost,\n",
    "## procedures_base_cost,providers_utilization,age\n",
    "payload = \"ambulatory,F,M,nonhispanic,white,F,72892002,185349003,118001005,210792.63,52916.21,129.16,69.16,129.16,0,14228,63\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept=CONTENT_TYPE_CSV)\n",
    "print(predictor.predict(payload))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different payload\n",
    "Now let's update the request with different values for encounter_class, procedure_code, encounter_code, gender, patient_healthcare_expenses, etc. and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"ambulatory,F,M,hispanic,white,F,72892002,185349003,118001005,210792.63,52916.21,129.16,69.16,129.16,0,14228,63\"\n",
    "\n",
    "print(predictor.predict(payload))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Deleting the Endpoint\n",
    "If you do not plan to use this endpoint, then it is a good practice to delete the endpoint so that you do not incur the cost of running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = sess.boto_session\n",
    "sm_client = boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
