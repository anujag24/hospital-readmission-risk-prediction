{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all cause 30-day hospital readmission risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/32/ce1926f05679ea5448fd3b98fbd9419d8c7a65f87d1a12ee5fb9577e3a8e/pyarrow-0.15.1-cp36-cp36m-manylinux2010_x86_64.whl (59.2MB)\n",
      "\u001b[K     |████████████████████████████████| 59.2MB 71.7MB/s eta 0:00:01    |████████████▋                   | 23.3MB 5.8MB/s eta 0:00:07     |████████████████████▍           | 37.8MB 5.8MB/s eta 0:00:04��██▉| 58.8MB 71.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.0.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from pyarrow) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from pyarrow) (1.17.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs into the python environment. These functions will be referenced later in the notebook code.\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.filesystem import S3FSWrapper\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Put this when it's called\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create table for missing data analysis\n",
    "def draw_missing_data_table(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data*100\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "# Plot validation curve\n",
    "def plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n",
    "    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n",
    "    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n",
    "    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n",
    "    plt.grid() \n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='best') \n",
    "    plt.xlabel('Parameter') \n",
    "    plt.ylabel('Score') \n",
    "    plt.ylim(ylim)\n",
    "    \n",
    "    \n",
    "# Read single parquet file from S3\n",
    "def pd_read_s3_parquet(key, bucket, s3_client=None, **args):\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    pf = ParquetFile('myfile.parq')\n",
    "    return pd.read_parquet(io.BytesIO(obj['Body'].read()), **args)\n",
    "\n",
    "# Read multiple parquets from a folder on S3 generated by spark\n",
    "def pd_read_s3_multiple_parquets(filepath, bucket, s3=None, \n",
    "                                 s3_client=None, verbose=False, **args):\n",
    "    if not filepath.endswith('/'):\n",
    "        filepath = filepath + '/'  # Add '/' to the end\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.client('s3')\n",
    "    if s3 is None:\n",
    "        s3 = boto3.resource('s3')\n",
    "    s3_keys = [item.key for item in s3.Bucket(bucket).objects.filter(Prefix=filepath)\n",
    "               if item.key.endswith('.parquet')]\n",
    "    if not s3_keys:\n",
    "        print('No parquet found in', bucket, filepath)\n",
    "    elif verbose:\n",
    "        print('Load parquets:')\n",
    "        for p in s3_keys: \n",
    "            print(p)\n",
    "    dfs = [pd_read_s3_parquet(key, bucket=bucket, s3_client=s3_client, **args) \n",
    "           for key in s3_keys]\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand the relationship between different tables and the data in those tables. This is important to identify the information which is relevant to the prediction. Here is the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Visualizing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EHR.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore \n",
    "import io\n",
    "bucket = 'readmission-data-ehr' # Update this to the bucket that was created in your lab account as part of this enviroment.\n",
    "prefix = \"train_data/2020/2/5\"\n",
    " \n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data files are in an S3 bucket in your AWS lab account. Twelve tables will be used (allergies, careplans, conditions,encounters, imaging-studies,immunizations, medications,observations, organizations, procedures, providers). Raw comma-separated value files will be downloaded into your Amazon Sagemaker instance, and imported into a DataFrame, where it's easier to work with the structured data. Raw files do not contain row headers, and thus labels are being assiged at import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-55ab1300dc9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##df_first_merge = pd_read_s3_multiple_parquets(prefix, bucket)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3FSWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "##df_first_merge = pd_read_s3_multiple_parquets(prefix, bucket)\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.filesystem import S3FSWrapper\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# Python 3.6 or later\n",
    "p_dataset = pq.ParquetDataset(\n",
    "    f\"s3://{bucket}/{prefix}\",\n",
    "    filesystem=fs\n",
    ")\n",
    "df_first_merge = p_dataset.read().to_pandas()\n",
    "\n",
    "df_first_merge.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: Taking the data in which encounter class is inpatient. For all other encounter class, patient is assumed to not be admitted in the hospital.\n",
    "Data preparation for cleaning and augmentation.\n",
    "Identifying the duration of Patient Admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_missing_data_table(df_first_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_first_merge = df_first_merge[df_first_merge['encounters_encounterclass'] == 'inpatient'] \n",
    "## Selecting data - Ignoring wellness (since patient not admitted)\n",
    "df_first_merge = df_first_merge[df_first_merge['encounters_encounterclass'].isin(['inpatient','ambulatory','urgentcare','outpatient','emergency'])] \n",
    "df_first_merge[\"encounters_stop\"] = pd.to_datetime(df_first_merge[\"encounters_stop\"]).dt.tz_localize(None)\n",
    "df_first_merge[\"encounters_start\"] = pd.to_datetime(df_first_merge[\"encounters_start\"]).dt.tz_localize(None)\n",
    "df_first_merge[\"admission_duration\"] = df_first_merge['encounters_stop'] - df_first_merge['encounters_start']\n",
    "df_first_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Immuninzations and Encounters to single data set based on Encounter ID since there are encounters for which there was no immunization administered. Left Join to take existing data merged with Immunization data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify missing values in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "df_first_merge.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_missing_data_table(df_first_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "## Imputation of missing data \n",
    "marital_status = 'NM'\n",
    "df_first_merge['patient_marital'] = df_first_merge['patient_marital'].fillna(marital_status, inplace=False)\n",
    "\n",
    "## There are entries in which procedures_code is missing, reason could be that no procedures were performed \n",
    "## so setting the value as zero for those outliers \n",
    "df_first_merge['procedures_code'] = df_first_merge['procedures_code'].fillna(0, inplace=False)\n",
    "\n",
    "## Setting the procedures base cost to zero since there was no procedure performed and no charge shown\n",
    "df_first_merge['procedures_base_cost'] = df_first_merge['procedures_base_cost'].fillna(0, inplace=False) \n",
    "\n",
    "## Due to low missing values for Patient ZIP, deleting those rows\n",
    "draw_missing_data_table(df_first_merge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting the data by Patient Id and Encounter Start Date so that we can calculate readmission within x number of days. \n",
    "##After looking at the data, it was found that there are multiple entries for encounter on the same start date so adding \n",
    "## encounter stop date to the sort\n",
    "df_first_merge = df_first_merge.sort_values(by=['patient_id', 'encounters_start', 'encounters_stop'])\n",
    "df_first_merge.describe(include='all')\n",
    "df_first_merge.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_merge['readmission_hrs'] = np.where((df_first_merge['patient_id'].shift(axis=0,periods=-1) == df_first_merge['patient_id']),(df_first_merge['encounters_start'].shift(axis=0,periods=-1) - df_first_merge['encounters_stop']).astype('timedelta64[h]'),0)\n",
    "## Shift the values of READMISSION by 1 row down\n",
    "df_first_merge['readmission_hrs'] = df_first_merge.apply(lambda x: df_first_merge['readmission_hrs'].shift(1))\n",
    "## Verifying data \n",
    "df_first_merge.head(10)\n",
    "\n",
    "## Convert ADMISSION DURATION into hours for model fitting\n",
    "td = df_first_merge['admission_duration']\n",
    "df_first_merge['admission_duration_hrs'] = td / pd.Timedelta('1 hour')\n",
    "\n",
    "df_first_merge.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Droppig rows which have nan in readmission_hrs column\n",
    "df_first_merge=df_first_merge.dropna(subset=['readmission_hrs'])\n",
    "## Identifying null values for readmission columns to validate calculation\n",
    "readmission_hrs_nv = pd.isnull(df_first_merge[\"readmission_hrs\"])\n",
    "df_first_merge[readmission_hrs_nv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to replace the negatives values with 0\n",
    "## Creating the label 'READMISSION' based on READMISSION criteria which is 30 days i.e. 720 hours\n",
    "## Readmission = 1 (true) if READMISSION_HRS < 720\n",
    "df_first_merge['readmission'] = np.where(((df_first_merge['readmission_hrs'] > 0) & (df_first_merge['readmission_hrs'] < 720)) ,1,0)\n",
    "##draw_missing_data_table(df_third_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether readmission occured for any medication encounters or not\n",
    "#df_first_merge[(df_first_merge['medications_totalcost'].isnull()) \n",
    "#               & (df_first_merge['readmission'] == 1)]\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether readmission occured for any medication encounters or not\n",
    "#df_first_merge[(df_first_merge['medications_totalcost'].notnull()) \n",
    "#               & (df_first_merge['readmission'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of days between readmissions if they exist\n",
    "# this only works for non-null values so you have to filter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_first_merge.loc[(df_first_merge['readmission_hrs'].notnull()) & \n",
    "                            (df_first_merge['readmission_hrs'] > 0),'readmission_hrs'], bins =range(0,800,20))\n",
    "plt.xlim([0,800])\n",
    "plt.xlabel('Hours between admissions for different encounters')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data to plot\n",
    "labels = 'Readmitted', 'Not Readmitted'\n",
    "sizes = [(df_first_merge.readmission == 1).sum(), (df_first_merge.readmission == 0).sum()]\n",
    "colors = ['red', 'green']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing Label Data i.e. Readmission \n",
    "print('Precent of positive samples:', (((df_first_merge.readmission == 1).sum())/len(df_first_merge)*100))\n",
    "print('Percent of negative samples:',  (((df_first_merge.readmission == 0).sum())/len(df_first_merge)*100))\n",
    "print('Total samples:',len(df_first_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_merge['patient_birthdate']= pd.to_datetime(df_first_merge['patient_birthdate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "def calculate_age(born):\n",
    "    today = date.today()\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "## Convert BIRTH DATE INTO AGE factor\n",
    "df_first_merge['age'] = df_first_merge['patient_birthdate'].apply(calculate_age)\n",
    "draw_missing_data_table(df_first_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the columns which are not required to do feature selection\n",
    "df_first_merge=df_first_merge.drop(columns=['admission_duration','medications_totalcost','medications_base_cost',\n",
    "                                          'patient_birthdate','encounters_id','patient_id','organizations_id',\n",
    "                                         'encounters_start','encounters_stop','admission_duration_hrs',\n",
    "                                            'encounters_payer','readmission_hrs'])\n",
    "\n",
    "## Dropping READMISSION_HRS since it affects the model negatively and takes away the importance of all other features\n",
    "#df_ft_select=df_ft_select.drop(columns=['readmission_hrs'])\n",
    "df_first_merge.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying categorical variables in the data set i.e. enumerated values for variables such as Gender, Organization State,Encounter Class, Encounter Code, Encounter Cost, Marital Status\n",
    "### We are identifying unique values for a column, if unique values are less than 25 we consider it as categorical variable\n",
    "df_first_merge['encounters_encounterclass'] = pd.Categorical(df_first_merge['encounters_encounterclass'])\n",
    "df_first_merge['patient_gender'] = pd.Categorical(df_first_merge['patient_gender'])\n",
    "df_first_merge['patient_marital'] = pd.Categorical(df_first_merge['patient_marital'])\n",
    "df_first_merge['patient_ethnicity'] = pd.Categorical(df_first_merge['patient_ethnicity'])\n",
    "df_first_merge['patient_race'] = pd.Categorical(df_first_merge['patient_race'])\n",
    "df_first_merge['providers_speciality'] = pd.Categorical(df_first_merge['providers_speciality'])\n",
    "df_first_merge['providers_state'] = pd.Categorical(df_first_merge['providers_state'])\n",
    "df_first_merge['encounters_reasoncode'] = pd.Categorical(df_first_merge['encounters_reasoncode'])\n",
    "df_first_merge['encounters_code'] = pd.Categorical(df_first_merge['encounters_code'])\n",
    "df_first_merge['procedures_code'] = pd.Categorical(df_first_merge['procedures_code'])\n",
    "\n",
    "\n",
    "# Transform categorical variables into dummy variables\n",
    "df_first_merge = pd.get_dummies(df_first_merge, drop_first=True)  # To avoid dummy trap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulted DataFrame is serialized and written to a flat file called the Pickle using the **Pickle** library. This file is then saved into the Amazon S3 bucket for later re-use of the data. You can generate a Pickle file using `pickle.dump` and save the raw datafile in that object and upload the file to the lab S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As a next step, we can use trusted pickle to check for signature on pickle dump when it is read to avoid spoofing\n",
    "import pickle\n",
    "with open('readmission-processed-data.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_first_merge, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "s3.Bucket(bucket).upload_file('readmission-processed-data.pkl','processed-data/readmission-processed-data.pkl',ExtraArgs={\"ServerSideEncryption\": \"aws:kms\",\"SSEKMSKeyId\":\"3a90a5d2-2ba8-4942-b9df-9a27ff7bf412\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
